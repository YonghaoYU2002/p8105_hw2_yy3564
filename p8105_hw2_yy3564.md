p8105_hw1_yy3564
================
Yonghao YU
2024-09-28

# Problem 2:

``` r
library(tidyverse)
library(readxl)
```

## First, read and clean the Mr.Trash Wheel dataset!

``` r
Mrtrashwheel_df = 
  read_excel("data/202309 Trash Wheel Collection Data.xlsx",
             sheet = 1, 
             range = "A2:N587",
             na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  mutate(year = as.numeric(year)) |>
  filter(weight_tons != 0, volume_cubic_yards != 0) |>
  drop_na(weight_tons:homes_powered) |>
  mutate(sports_balls_modified = as.integer(round(sports_balls, 0)))|>
  select(-sports_balls)
Mrtrashwheel_df
```

    ## # A tibble: 585 × 14
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 575 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, sports_balls_modified <int>

## Then, read and clean the Professor Trash Wheel dataset!

``` r
Proftrashwheel_df = 
  read_excel("data/202309 Trash Wheel Collection Data.xlsx",
             sheet = 2, 
             range = "A2:M109",
             na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  filter(weight_tons != 0, volume_cubic_yards != 0) |>
  drop_na(weight_tons:homes_powered)

Proftrashwheel_df
```

    ## # A tibble: 104 × 13
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ##  2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ##  3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ##  4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ##  5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ##  6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ##  7        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ##  8        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ##  9        9 May       2017 2017-05-10 00:00:00        2.64                 15
    ## 10       10 May       2017 2017-05-26 00:00:00        2.78                 15
    ## # ℹ 94 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>

## Last, read and clean the Gwynnda Trash Wheel dataset!

``` r
Gwytrashwheel_df = 
  read_excel("data/202309 Trash Wheel Collection Data.xlsx",
             sheet = 4, 
             range = "A2:L159",
             na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  filter(weight_tons != 0, volume_cubic_yards != 0) |>
  drop_na(weight_tons:homes_powered)

Gwytrashwheel_df
```

    ## # A tibble: 39 × 12
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ##  1      117 November  2022 2022-11-03 00:00:00        3.06                 15
    ##  2      118 November  2022 2022-11-15 00:00:00        3                    15
    ##  3      119 November  2022 2022-11-19 00:00:00        2.42                 15
    ##  4      120 November  2022 2022-11-22 00:00:00        2.37                 15
    ##  5      121 November  2022 2022-11-30 00:00:00        2.91                 15
    ##  6      122 December  2022 2022-12-13 00:00:00        2.35                 14
    ##  7      123 December  2022 2022-12-17 00:00:00        2.8                  15
    ##  8      124 December  2022 2022-12-17 00:00:00        2.69                 15
    ##  9      125 December  2022 2022-12-19 00:00:00        2.27                 15
    ## 10      126 December  2022 2022-12-19 00:00:00        2.5                  15
    ## # ℹ 29 more rows
    ## # ℹ 6 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>

## Before combining, add an additional column indicating which Trash Wheel is which!

``` r
Mrtrashwheel_df = mutate(Mrtrashwheel_df, which_trash_wheel = "Mr.Trash Wheel")
Proftrashwheel_df = mutate(Proftrashwheel_df, which_trash_wheel = "Professor Trash Wheel")
Gwytrashwheel_df = mutate(Gwytrashwheel_df, which_trash_wheel = "Gwynnda Trash Wheel")
```

## Then find a way to combine the datasets,for “sports_balls_modified is NA”, convert it into 0.

``` r
combined_df = 
  bind_rows(Mrtrashwheel_df, Proftrashwheel_df, Gwytrashwheel_df) |>
  mutate(sports_balls_modified = ifelse(is.na(sports_balls_modified), 0, sports_balls_modified))
combined_df 
```

    ## # A tibble: 728 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 718 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, sports_balls_modified <dbl>,
    ## #   which_trash_wheel <chr>

## Finally, we use inline R to write a paragraph about this data!!!

There are 725 observations in the combined data.

From available data, the total weight of trash collected by Professor
Trash Wheel is 1875.1 tons.

And from available data, the total number of cigarette butts collected
by Gwynnda in June of 2022 is 0.

I think it may because when we do data cleaning, the Gwynnda Trash Wheel
dataset contain rows that do not include dumpster-specific data(June of
2022 is also in this range),and I discard those rows of data, so when I
try to find the number of cigarette butts in the combined dataset, those
part disappear, hence the value is 0!

# Problem 3:

## First, we do data cleaning for three datasets seperately!

## For bakers.csv dataset:

``` r
bakers_df = 
  read_csv("data/bakers.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  distinct()  
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakers_df  
```

    ## # A tibble: 120 × 5
    ##    baker_name       series baker_age baker_occupation             hometown      
    ##    <chr>             <dbl>     <dbl> <chr>                        <chr>         
    ##  1 Ali Imdad             4        25 Charity worker               Saltley, Birm…
    ##  2 Alice Fevronia       10        28 Geography teacher            Essex         
    ##  3 Alvin Magallanes      6        37 Nurse                        Bracknell, Be…
    ##  4 Amelia LeBruin       10        24 Fashion designer             Halifax       
    ##  5 Andrew Smyth          7        25 Aerospace engineer           Derby / Holyw…
    ##  6 Annetha Mills         1        30 Midwife                      Essex         
    ##  7 Antony Amourdoux      9        30 Banker                       London        
    ##  8 Beca Lyne-Pirkis      4        31 Military Wives' Choir Singer Aldershot, Ha…
    ##  9 Ben Frazer            2        31 Graphic Designer             Northampton   
    ## 10 Benjamina Ebuehi      7        23 Teaching assistant           South London  
    ## # ℹ 110 more rows

## For bakes.csv dataset:

``` r
bakes_df = 
  read_csv("data/bakes.csv", na = c("NA", ".", "", "UNKNOWN", "N/A", "Unknown")) |>
  janitor::clean_names() |>
  distinct() |> 
  mutate(baker = ifelse(baker == "\"Jo\"", "Jo", baker)) 
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df  
```

    ## # A tibble: 548 × 5
    ##    series episode baker     signature_bake                          show_stopper
    ##     <dbl>   <dbl> <chr>     <chr>                                   <chr>       
    ##  1      1       1 Annetha   "Light Jamaican Black Cakewith Strawbe… Red, White …
    ##  2      1       1 David     "Chocolate Orange Cake"                 Black Fores…
    ##  3      1       1 Edd       "Caramel Cinnamon and Banana Cake"      <NA>        
    ##  4      1       1 Jasminder "Fresh Mango and Passion Fruit Humming… <NA>        
    ##  5      1       1 Jonathan  "Carrot Cake with Lime and Cream Chees… Three Tiere…
    ##  6      1       1 Lea       "Cranberry and Pistachio Cakewith Oran… Raspberries…
    ##  7      1       1 Louise    "Carrot and Orange Cake"                Never Fail …
    ##  8      1       1 Mark      "Sticky Marmalade Tea Loaf"             Heart-shape…
    ##  9      1       1 Miranda   "Triple Layered Brownie Meringue Cake\… Three Tiere…
    ## 10      1       1 Ruth      "Three Tiered Lemon Drizzle Cakewith F… Classic Cho…
    ## # ℹ 538 more rows

## Finally, for results.csv dataset:

``` r
results_df = 
  read_csv("data/results.csv", na = c("NA", ".", "", "Unknown", "UNKNOWN", "N/A"), skip = 2) |>
  janitor::clean_names() |>
  distinct()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df  
```

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## # ℹ 1,126 more rows

## Use anti_join to check discrepancies between bakers and bakes datasets

``` r
bakers_df_updated = bakers_df |>
  mutate(baker = sub(" .*", "", baker_name)) |>
  select(series,baker,everything())
missing_in_bakers1 = anti_join(bakes_df, bakers_df_updated, by = c("series", "baker"))
missing_in_bakes1 = anti_join(bakers_df_updated, bakes_df, by = c("series", "baker"))
```

## Use anti_join to check discrepancies between bakes and results datasets

``` r
missing_in_results1 = anti_join(bakes_df, results_df, by = c("series", "episode", "baker"))
missing_in_bakes2 = anti_join(results_df, bakes_df, by = c("series", "episode", "baker"))
missing_in_results1
```

    ## # A tibble: 8 × 5
    ##   series episode baker signature_bake                               show_stopper
    ##    <dbl>   <dbl> <chr> <chr>                                        <chr>       
    ## 1      2       1 Jo    Chocolate Orange CupcakesOrange and Cardamo… Chocolate a…
    ## 2      2       2 Jo    Caramelised Onion, Gruyere and Thyme Quiche  Raspberry a…
    ## 3      2       3 Jo    Stromboli flavored with Mozzarella, Ham, an… <NA>        
    ## 4      2       4 Jo    Lavender Biscuits                            Blueberry M…
    ## 5      2       5 Jo    Salmon and Asparagus Pie                     Apple and R…
    ## 6      2       6 Jo    Rum and Raisin Baked Cheesecake              Limoncello …
    ## 7      2       7 Jo    Raspberry & Strawberry Mousse Cake           Pain Aux Ra…
    ## 8      2       8 Jo    Raspberry and Blueberry Mille Feuille        Mini Victor…

## Use anti_join to check discrepancies between bakers and results datasets

``` r
missing_in_results2 = anti_join(bakers_df_updated, results_df, by = c("series", "baker"))
missing_in_bakers2 = anti_join(results_df, bakers_df_updated, by = c("series", "baker"))
missing_in_bakers2
```

    ## # A tibble: 8 × 5
    ##   series episode baker  technical result    
    ##    <dbl>   <dbl> <chr>      <dbl> <chr>     
    ## 1      2       1 Joanne        11 IN        
    ## 2      2       2 Joanne        10 IN        
    ## 3      2       3 Joanne         1 IN        
    ## 4      2       4 Joanne         8 IN        
    ## 5      2       5 Joanne         6 IN        
    ## 6      2       6 Joanne         1 STAR BAKER
    ## 7      2       7 Joanne         3 IN        
    ## 8      2       8 Joanne         1 WINNER

## Then merge three datasets together to create a single,and final dataset!

``` r
complete_dataset <- results_df |>
  full_join(bakes_df, by = c("series", "episode", "baker")) |>  
  full_join(bakers_df_updated, by = c("series", "baker"))
complete_dataset
```

    ## # A tibble: 1,144 × 11
    ##    series episode baker  technical result signature_bake show_stopper baker_name
    ##     <dbl>   <dbl> <chr>      <dbl> <chr>  <chr>          <chr>        <chr>     
    ##  1      1       1 Annet…         2 IN     "Light Jamaic… Red, White … Annetha M…
    ##  2      1       1 David          3 IN     "Chocolate Or… Black Fores… David Cha…
    ##  3      1       1 Edd            1 IN     "Caramel Cinn… <NA>         Edd Kimber
    ##  4      1       1 Jasmi…        NA IN     "Fresh Mango … <NA>         Jasminder…
    ##  5      1       1 Jonat…         9 IN     "Carrot Cake … Three Tiere… Jonathan …
    ##  6      1       1 Louise        NA IN     "Carrot and O… Never Fail … Louise Br…
    ##  7      1       1 Miran…         8 IN     "Triple Layer… Three Tiere… Miranda B…
    ##  8      1       1 Ruth          NA IN     "Three Tiered… Classic Cho… Ruth Clem…
    ##  9      1       1 Lea           10 OUT    "Cranberry an… Raspberries… Lea Harris
    ## 10      1       1 Mark          NA OUT    "Sticky Marma… Heart-shape… Mark Whit…
    ## # ℹ 1,134 more rows
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

## Export the final dataset as a CSV

``` r
write_csv(complete_dataset, "final_complete_dataset.csv")
```

## Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset:

1.  Describe the data cleaning process: First of all, I read the data
    from the bakers.csv file, handles missing values, converts column
    names to lowercase and standardizes the format, and finally detects
    and removes duplicate rows to create a cleaned data frame bakers_df.
    Then, I read the data from the bakes.csv file, treats specified
    values (like “NA”, “.”, “UNKNOWN”, etc.) as missing, cleans and
    standardizes the column names, removes duplicate rows, and replaces
    the value “Jo” (if it appears with double quotes) in the baker
    column with the value of the Jo. The cleaned and modified data is
    stored in bakes_df for later use. After that, I read data from the
    results.csv file, with skipping the first two rows, and treats
    specific values (such as “NA”, “.”, “Unknown”, “N/A”, etc.) as
    missing values. It then cleans and standardizes the column names,
    removes any duplicate rows, and stores the cleaned data in
    results_df for later use!

2.  Briefly discuss the final dataset: The final dataset is achieved by
    the full join of the three cleaned datasets, in the final dataset,
    there are 1144 rows and 11 columns.

## Then Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10:

``` r
obtable = filter(complete_dataset, series >= 5)|>
  filter(result == "STAR BAKER" | result == "WINNER") |>
  select(series, baker, result)
obtable
```

    ## # A tibble: 60 × 3
    ##    series baker   result    
    ##     <dbl> <chr>   <chr>     
    ##  1      5 Nancy   STAR BAKER
    ##  2      5 Richard STAR BAKER
    ##  3      5 Luis    STAR BAKER
    ##  4      5 Richard STAR BAKER
    ##  5      5 Kate    STAR BAKER
    ##  6      5 Chetna  STAR BAKER
    ##  7      5 Richard STAR BAKER
    ##  8      5 Richard STAR BAKER
    ##  9      5 Richard STAR BAKER
    ## 10      5 Nancy   WINNER    
    ## # ℹ 50 more rows

## Comment on this table – were there any predictable overall winners? Any surprises?

1.  Predictable Winners: We Look for bakers who consistently won Star
    Baker. If a baker frequently appeared as a Star Baker and then
    became the overall Winner, their victory would be predictable.

2.  Any Surprises: Any winner who did not frequently win Star Baker, or
    who only started winning in the later episodes, would be a surprise.
    Similarly, if a baker dominated the Star Baker titles but did not
    win the competition, this could also be unexpected.

## Then I will Import, clean, tidy, and organize the viewership data in viewers.csv. Then show the first 10 rows of this dataset!

``` r
viewers_df = 
  read_csv("data/viewers.csv", na = c("NA")) |>
  janitor::clean_names() |>
  distinct() |>
  pivot_longer(series_1:series_10, names_to = "series", values_to = "viewership") |>
  mutate(series = sub(".*_", "", series)) |>
  select(series, everything()) |>
  arrange(series, episode)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers_df, n = 10)
```

    ## # A tibble: 10 × 3
    ##    series episode viewership
    ##    <chr>    <dbl>      <dbl>
    ##  1 1            1       2.24
    ##  2 1            2       3   
    ##  3 1            3       3   
    ##  4 1            4       2.6 
    ##  5 1            5       3.03
    ##  6 1            6       2.75
    ##  7 1            7      NA   
    ##  8 1            8      NA   
    ##  9 1            9      NA   
    ## 10 1           10      NA

## After that, calculate the average viewership of Season 1:

``` r
sum1 = filter(viewers_df, series == 1) |>
  select(viewership) |>
  drop_na() |>
  sum() 
length = sapply(select(filter(viewers_df, series == 1), viewership), function(x) length(x)-sum(is.na(x)))
avg1 = sum1/unname(length)
avg1
```

    ## [1] 2.77

## Then use the same method to calculate the average viewership of Season 5:

``` r
sum5 = filter(viewers_df, series == 5) |>
  select(viewership) |>
  drop_na() |>
  sum() 
length = sapply(select(filter(viewers_df, series == 5), viewership), function(x) length(x)-sum(is.na(x)))
avg5 = sum5/unname(length)
avg5
```

    ## [1] 10.0393
